{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **----- Import Libraries -----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast, json\n",
    "\n",
    "# import the libraries for importing the model \n",
    "import pickle\n",
    "\n",
    "# import train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import the vectorizers, scaler, smote\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# import the model including logistic regression, decision tree, bagged decision tree, random forest, ada boost, gradient boost, xgboost, svm \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import the metrics including accuracy score, precision score, recall score, f1 score, roc auc score, confusion matrix as well as classification report, roc curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **----- Import Model -----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the file '../model/pantip_post_model_train_simplified_label_lr.pkl', so we can use the model to predict the new data\n",
    "with open('../model/pantip_post_model_train_simplified_label_lr.pkl', 'rb') as file:\n",
    "    pantip_post_model_train_simplified_label_lr = pickle.load(file)\n",
    "\n",
    "# import the file '../model/pantip_post_model_train_simplified_label_vectorizer.pkl', so we can use the vectorizer to transform the new data\n",
    "with open('../model/pantip_post_model_train_simplified_label_vectorizer.pkl', 'rb') as file:\n",
    "    pantip_post_model_train_simplified_label_vectorizer = pickle.load(file)\n",
    "\n",
    "# import the file '../model/pantip_post_model_train_simplified_label_scaler.pkl', so we can use the scaler to transform the new data\n",
    "with open('../model/pantip_post_model_train_simplified_label_scaler.pkl', 'rb') as file:\n",
    "    pantip_post_model_train_simplified_label_scaler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **----- Import Data -----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data from '../output - final/df_youtube_comment_suicide_labeled_processed.csv', so we can use the model to predict the new data\n",
    "df_youtube_comment_suicide_labeled_processed = pd.read_csv('../output - final/df_youtube_comment_suicide_labeled_processed.csv')\n",
    "\n",
    "# import the data from '../model/X_train_sm_columns.csv', so we can use the data in the dataframe to match the X_sm and use the model 'pantip_post_model_train_simplified_label_lr.pkl' to predict\n",
    "with open('../model/X_train_sm_label_columns.json') as f:\n",
    "    X_train_sm_columns = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **----- Define X,y  -----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13360, 15)\n",
      "Index(['Unnamed: 0', 'videoId', 'title', 'comment', 'date', 'label',\n",
      "       'sub_label', 'tokenized_text', 'day_week', 'day_month', 'month_year',\n",
      "       'year', 'time_day', 'text_len', 'text_emoji'],\n",
      "      dtype='object')\n",
      "label\n",
      "0    12883\n",
      "1      477\n",
      "Name: count, dtype: int64\n",
      "sub_label\n",
      "0     12883\n",
      "12      271\n",
      "11      206\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# list the column of the dataframe df_youtube_comment_suicide_labeled_processed \n",
    "print(df_youtube_comment_suicide_labeled_processed.shape)\n",
    "print(df_youtube_comment_suicide_labeled_processed.columns)\n",
    "print(df_youtube_comment_suicide_labeled_processed['label'].value_counts())\n",
    "print(df_youtube_comment_suicide_labeled_processed['sub_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13360, 8)\n",
      "(13360,)\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>day_week</th>\n",
       "      <th>day_month</th>\n",
       "      <th>month_year</th>\n",
       "      <th>year</th>\n",
       "      <th>time_day</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ขอบคุณ']</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>26</td>\n",
       "      <td>November</td>\n",
       "      <td>2023</td>\n",
       "      <td>9-12</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['อย่า', 'ยึด', 'ติดกับ', 'ความดี', 'ตีเส้น', ...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>10</td>\n",
       "      <td>November</td>\n",
       "      <td>2023</td>\n",
       "      <td>21-24</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ขอบคุณ']</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>7</td>\n",
       "      <td>September</td>\n",
       "      <td>2023</td>\n",
       "      <td>9-12</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['อีฟ', 'อ่านใจ']</td>\n",
       "      <td>Monday</td>\n",
       "      <td>7</td>\n",
       "      <td>August</td>\n",
       "      <td>2023</td>\n",
       "      <td>3-6</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ขอบคุณ']</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>23</td>\n",
       "      <td>July</td>\n",
       "      <td>2023</td>\n",
       "      <td>3-6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized_text  day_week  day_month  \\\n",
       "0                                         ['ขอบคุณ']    Sunday         26   \n",
       "1  ['อย่า', 'ยึด', 'ติดกับ', 'ความดี', 'ตีเส้น', ...    Friday         10   \n",
       "2                                         ['ขอบคุณ']  Thursday          7   \n",
       "3                                  ['อีฟ', 'อ่านใจ']    Monday          7   \n",
       "4                                         ['ขอบคุณ']    Sunday         23   \n",
       "\n",
       "  month_year  year time_day  text_len  text_emoji  \n",
       "0   November  2023     9-12        10           0  \n",
       "1   November  2023    21-24       130           0  \n",
       "2  September  2023     9-12        14           1  \n",
       "3     August  2023      3-6        37           0  \n",
       "4       July  2023      3-6        13           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define X,y which include the following columns ['tokenized_text', 'day_week', 'day_month', 'month_year', 'year', 'time_day', 'text_len', 'text_emoji']\n",
    "\n",
    "X = df_youtube_comment_suicide_labeled_processed[['tokenized_text', 'day_week', 'day_month', 'month_year', 'year', 'time_day', 'text_len', 'text_emoji']]\n",
    "y = df_youtube_comment_suicide_labeled_processed['label']\n",
    "\n",
    "# Check X,y\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y.head())\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **----- Feature Engineering  -----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['day_week_Monday', 'day_week_Saturday', 'day_week_Sunday',\n",
      "       'day_week_Thursday', 'day_week_Tuesday', 'day_week_Wednesday',\n",
      "       'day_month_2', 'day_month_3', 'day_month_4', 'day_month_5',\n",
      "       'day_month_6', 'day_month_7', 'day_month_8', 'day_month_9',\n",
      "       'day_month_10', 'day_month_11', 'day_month_12', 'day_month_13',\n",
      "       'day_month_14', 'day_month_15', 'day_month_16', 'day_month_17',\n",
      "       'day_month_18', 'day_month_19', 'day_month_20', 'day_month_21',\n",
      "       'day_month_22', 'day_month_23', 'day_month_24', 'day_month_25',\n",
      "       'day_month_26', 'day_month_27', 'day_month_28', 'day_month_29',\n",
      "       'day_month_30', 'day_month_31', 'month_year_August',\n",
      "       'month_year_December', 'month_year_February', 'month_year_January',\n",
      "       'month_year_July', 'month_year_June', 'month_year_March',\n",
      "       'month_year_May', 'month_year_November', 'month_year_October',\n",
      "       'month_year_September', 'year_2022', 'year_2023', 'time_day_12-15',\n",
      "       'time_day_15-18', 'time_day_18-21', 'time_day_21-24', 'time_day_3-6',\n",
      "       'time_day_6-9', 'time_day_9-12', 'text_emoji_1'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(13360, 57)\n"
     ]
    }
   ],
   "source": [
    "#Step 1: One-Hot Encoding for Categorical Data\n",
    "\n",
    "cols = ['day_week', 'day_month', 'month_year', 'year', 'time_day', 'text_emoji']\n",
    "X_categorical = pd.get_dummies(X[cols], columns=cols, drop_first=True, dtype=int)\n",
    "\n",
    "print(X_categorical.columns)\n",
    "print(type(X_categorical))\n",
    "print(X_categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13360, 17505)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2: use the imported vectorizer to vecdtorize the data\n",
    "\n",
    "X_text = pantip_post_model_train_simplified_label_vectorizer.transform(X['tokenized_text'])\n",
    "\n",
    "print(type(X_text))\n",
    "X_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13360, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: use the imported scaler to scaling the data\n",
    "\n",
    "X_numerical = pantip_post_model_train_simplified_label_scaler.transform(X[['text_len']])\n",
    "\n",
    "print(type(X_numerical))\n",
    "X_numerical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13360, 17563)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Concatenate All Features for Training Data\n",
    "\n",
    "X_all = pd.concat([\n",
    "    pd.DataFrame(X_numerical, columns=['text_len'], index=X.index),\n",
    "    pd.DataFrame(X_text.todense(), columns=pantip_post_model_train_simplified_label_vectorizer.get_feature_names_out(), index=X.index),\n",
    "    X_categorical\n",
    "], axis=1)\n",
    "\n",
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13360, 17563)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Checking and Renaming Duplicate Features\n",
    "\n",
    "# Function to rename duplicate columns by adding a suffix\n",
    "def rename_duplicates(df):\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique():\n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '_' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "# Apply it to your training data\n",
    "rename_duplicates(X_all)\n",
    "\n",
    "# check the shape of X_all\n",
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13360, 17563)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['text_len',\n",
       " '__',\n",
       " '___',\n",
       " '____',\n",
       " '_____',\n",
       " '________',\n",
       " '_________',\n",
       " '__________',\n",
       " '____________',\n",
       " '__________________',\n",
       " '________________________',\n",
       " '_____________________________________',\n",
       " '______________________________________',\n",
       " '___________________________________________',\n",
       " '____________________________________________',\n",
       " '____________________________________________________',\n",
       " '______________________________________________________',\n",
       " '______________________________________________________________',\n",
       " '_______________________________________________________________',\n",
       " '______________________________________________________________________',\n",
       " '____________________________________________________________________________________',\n",
       " '_____________________________________________________________________________________________',\n",
       " '____________________________________________________________________________________________________',\n",
       " '______________________________________________________________________________________________________________',\n",
       " '__า',\n",
       " '_ผม',\n",
       " '_ผมม',\n",
       " '_ย',\n",
       " '_วย',\n",
       " '_หว',\n",
       " '_อ',\n",
       " '_อก',\n",
       " 'aa',\n",
       " 'ab',\n",
       " 'abac',\n",
       " 'abandoned',\n",
       " 'abc',\n",
       " 'abilify',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abusive',\n",
       " 'aca',\n",
       " 'academy',\n",
       " 'accept',\n",
       " 'acceptance',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'account',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'acid',\n",
       " 'acnotin',\n",
       " 'action',\n",
       " 'actionelement',\n",
       " 'activation',\n",
       " 'active',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'addeventlistener',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'address',\n",
       " 'adha',\n",
       " 'adhd',\n",
       " 'adjective',\n",
       " 'adjust',\n",
       " 'adjustment',\n",
       " 'admid',\n",
       " 'admin',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'adobe',\n",
       " 'advance',\n",
       " 'advice',\n",
       " 'advisor',\n",
       " 'ae',\n",
       " 'aec',\n",
       " 'aer',\n",
       " 'aero',\n",
       " 'affects',\n",
       " 'affirmation',\n",
       " 'afraid',\n",
       " 'afriad',\n",
       " 'after',\n",
       " 'again',\n",
       " 'age',\n",
       " 'agentur',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'agly',\n",
       " 'ago',\n",
       " 'agolaphobia',\n",
       " 'agomelatine',\n",
       " 'agoraphobia',\n",
       " 'ai',\n",
       " 'aids',\n",
       " 'aim',\n",
       " 'aimer',\n",
       " 'air',\n",
       " 'airpods',\n",
       " 'ais',\n",
       " 'akara',\n",
       " 'aki',\n",
       " 'ala',\n",
       " 'alcaraz',\n",
       " 'alcohol',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'algorithm',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allen',\n",
       " 'allergic',\n",
       " 'allow',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'alpha',\n",
       " 'alphamethyldopa',\n",
       " 'alprazolam',\n",
       " 'alprazolem',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alsthom',\n",
       " 'altered',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'altkey',\n",
       " 'altogether',\n",
       " 'always',\n",
       " 'alzeimer',\n",
       " 'am',\n",
       " 'amalgam',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'ambivert',\n",
       " 'american',\n",
       " 'amilykatze',\n",
       " 'amitrip',\n",
       " 'amitripline',\n",
       " 'amitriptline',\n",
       " 'amitriptyline',\n",
       " 'amitrptyline',\n",
       " 'amlodipine',\n",
       " 'amounts',\n",
       " 'amoxiciliin',\n",
       " 'amoxicillin',\n",
       " 'amt',\n",
       " 'amygdala',\n",
       " 'amz',\n",
       " 'an',\n",
       " 'anafranil',\n",
       " 'anal',\n",
       " 'analfranil',\n",
       " 'analysis',\n",
       " 'anatomy',\n",
       " 'and',\n",
       " 'andrey',\n",
       " 'android',\n",
       " 'androphobia',\n",
       " 'anger',\n",
       " 'anhe',\n",
       " 'anhedonia',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animation',\n",
       " 'anime',\n",
       " 'animetion',\n",
       " 'aniwat',\n",
       " 'anne',\n",
       " 'anoraxianavosa',\n",
       " 'anorexia',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'anta',\n",
       " 'antagonist',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'antibiotic',\n",
       " 'antibody',\n",
       " 'antidepressant',\n",
       " 'antidepressants',\n",
       " 'antigen',\n",
       " 'antiphospholipid',\n",
       " 'antisocial',\n",
       " 'anxiety',\n",
       " 'anxiset',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyconfidence',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anywhere',\n",
       " 'anziety',\n",
       " 'ao',\n",
       " 'ap',\n",
       " 'apalife',\n",
       " 'apink',\n",
       " 'apnea',\n",
       " 'apollo',\n",
       " 'app',\n",
       " 'appearance',\n",
       " 'appetite',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'approved',\n",
       " 'april',\n",
       " 'ar',\n",
       " 'arachnophobia',\n",
       " 'arbeit',\n",
       " 'arcamone',\n",
       " 'arden',\n",
       " 'are',\n",
       " 'area',\n",
       " 'arfid',\n",
       " 'aricept',\n",
       " 'aripiprazole',\n",
       " 'arnulfo',\n",
       " 'around',\n",
       " 'arphatnok',\n",
       " 'arrest',\n",
       " 'art',\n",
       " 'arts',\n",
       " 'arwt',\n",
       " 'as',\n",
       " 'asana',\n",
       " 'ashwagandha',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asleep',\n",
       " 'aspartate',\n",
       " 'aspect',\n",
       " 'asperger',\n",
       " 'aspergers',\n",
       " 'ass',\n",
       " 'assaults',\n",
       " 'assessment',\n",
       " 'assisi',\n",
       " 'assist',\n",
       " 'assistant',\n",
       " 'assisted',\n",
       " 'assure',\n",
       " 'astrazeneca',\n",
       " 'astrology',\n",
       " 'astronaut',\n",
       " 'asus',\n",
       " 'at',\n",
       " 'atelophobia',\n",
       " 'ativan',\n",
       " 'atk',\n",
       " 'atm',\n",
       " 'atopic',\n",
       " 'attach',\n",
       " 'attachevent',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attivan',\n",
       " 'atyp',\n",
       " 'atypical',\n",
       " 'audioslave',\n",
       " 'ausbildung',\n",
       " 'ausl',\n",
       " 'australia',\n",
       " 'authentic',\n",
       " 'author',\n",
       " 'authuser',\n",
       " 'autism',\n",
       " 'autistic',\n",
       " 'automatic',\n",
       " 'automotive',\n",
       " 'auxclick',\n",
       " 'avenger',\n",
       " 'average',\n",
       " 'avicii',\n",
       " 'avocado',\n",
       " 'avoid',\n",
       " 'avoidance',\n",
       " 'avoidant',\n",
       " 'avp',\n",
       " 'awake',\n",
       " 'awakening',\n",
       " 'away',\n",
       " 'aways',\n",
       " 'awful',\n",
       " 'axon',\n",
       " 'az',\n",
       " 'ba',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'backend',\n",
       " 'background',\n",
       " 'backlash',\n",
       " 'backup',\n",
       " 'bacteria',\n",
       " 'bacteriophobia',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'bahaviors',\n",
       " 'bahn',\n",
       " 'balance',\n",
       " 'ballybricken',\n",
       " 'band',\n",
       " 'bangkok',\n",
       " 'bangsaen',\n",
       " 'bangtan',\n",
       " 'bangtangayo',\n",
       " 'bank',\n",
       " 'banking',\n",
       " 'banzanac',\n",
       " 'bar',\n",
       " 'barbiturates',\n",
       " 'barden',\n",
       " 'bare',\n",
       " 'bargain',\n",
       " 'bargaining',\n",
       " 'barista',\n",
       " 'barks',\n",
       " 'barry',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basil',\n",
       " 'basis',\n",
       " 'battle',\n",
       " 'bbq',\n",
       " 'bc',\n",
       " 'bdd',\n",
       " 'bdsm',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'bear',\n",
       " 'bearable',\n",
       " 'beast',\n",
       " 'beaten',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'because',\n",
       " 'beck',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'beetalk',\n",
       " 'before',\n",
       " 'begin',\n",
       " 'behavior',\n",
       " 'behavioral',\n",
       " 'behaviour',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'beings',\n",
       " 'believe',\n",
       " 'bell',\n",
       " 'belong',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'bennington',\n",
       " 'benz',\n",
       " 'benzhexol',\n",
       " 'benzodiazepine',\n",
       " 'benzodiazepines',\n",
       " 'berningtonlinkinpark',\n",
       " 'best',\n",
       " 'beta',\n",
       " 'betahistine',\n",
       " 'betalol',\n",
       " 'better',\n",
       " 'between',\n",
       " 'bi',\n",
       " 'bieber',\n",
       " 'bierut',\n",
       " 'big',\n",
       " 'bigbang',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bilingual',\n",
       " 'billie',\n",
       " 'billionaire',\n",
       " 'bind',\n",
       " 'binge',\n",
       " 'bio',\n",
       " 'biochem',\n",
       " 'biological',\n",
       " 'bipolar',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bisexual',\n",
       " 'bisopolol',\n",
       " 'bit',\n",
       " 'biw',\n",
       " 'black',\n",
       " 'blacklist',\n",
       " 'blackmail',\n",
       " 'blackmay',\n",
       " 'blackmores',\n",
       " 'blackpink',\n",
       " 'blame',\n",
       " 'blank',\n",
       " 'bleeding',\n",
       " 'blender',\n",
       " 'blink',\n",
       " 'blissiam',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blocker',\n",
       " 'blog',\n",
       " 'bloggang',\n",
       " 'blogger',\n",
       " 'blogspot',\n",
       " 'blonde',\n",
       " 'bloom',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'blur',\n",
       " 'bm',\n",
       " 'bmw',\n",
       " 'bnh',\n",
       " 'bnk',\n",
       " 'body',\n",
       " 'bodys',\n",
       " 'bodyslam',\n",
       " 'bohemain',\n",
       " 'bolbbalgan',\n",
       " 'bolt',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'bonne',\n",
       " 'bonus',\n",
       " 'bonvoyage',\n",
       " 'book',\n",
       " 'bookiezz',\n",
       " 'books',\n",
       " 'booms',\n",
       " 'boost',\n",
       " 'borderline',\n",
       " 'born',\n",
       " 'boss',\n",
       " 'bossy',\n",
       " 'boston',\n",
       " 'both',\n",
       " 'bottles',\n",
       " 'bow',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boycott',\n",
       " 'boyfriend',\n",
       " 'bp',\n",
       " 'bpd',\n",
       " 'bradycardia',\n",
       " 'brain',\n",
       " 'brainstorm',\n",
       " 'braintellix',\n",
       " 'brainwave',\n",
       " 'brand',\n",
       " 'branding',\n",
       " 'brandname',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakdowns',\n",
       " 'breast',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breezy',\n",
       " 'brent',\n",
       " 'brian',\n",
       " 'bridge',\n",
       " 'brief',\n",
       " 'bright',\n",
       " 'brighter',\n",
       " 'bring',\n",
       " 'brintellix',\n",
       " 'broken',\n",
       " 'bromance',\n",
       " 'brompheniramine',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brownout',\n",
       " 'browser',\n",
       " 'brt',\n",
       " 'bruh',\n",
       " 'bs',\n",
       " 'btob',\n",
       " 'bts',\n",
       " 'bu',\n",
       " 'bubbles',\n",
       " 'buddhism',\n",
       " 'buddhist',\n",
       " 'buf',\n",
       " 'bulimia',\n",
       " 'bullet',\n",
       " 'bullie',\n",
       " 'bullied',\n",
       " 'bullimia',\n",
       " 'bullmore',\n",
       " 'bully',\n",
       " 'bullying',\n",
       " 'bun',\n",
       " 'bupropion',\n",
       " 'burden',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burnout',\n",
       " 'bury',\n",
       " 'business',\n",
       " 'businessinsider',\n",
       " 'bussiness',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'butterfly',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buzan',\n",
       " 'buzzfeed',\n",
       " 'by',\n",
       " 'byyyynaaaa',\n",
       " 'bzd',\n",
       " 'bzds',\n",
       " 'ca',\n",
       " 'cable',\n",
       " 'cacioppo',\n",
       " 'cad',\n",
       " 'cadaver',\n",
       " 'cafe',\n",
       " 'cal',\n",
       " 'californication',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calm',\n",
       " 'calvin',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cancel',\n",
       " 'candidate',\n",
       " 'candy',\n",
       " 'cannabidol',\n",
       " 'cannabis',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cap',\n",
       " 'capable',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'carbonate',\n",
       " 'card',\n",
       " 'cardiac',\n",
       " 'cardiology',\n",
       " 'care',\n",
       " 'cared',\n",
       " 'career',\n",
       " 'carl',\n",
       " 'carlos',\n",
       " 'carnegie',\n",
       " 'carol',\n",
       " 'carrie',\n",
       " 'carried',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'cartoon',\n",
       " 'case',\n",
       " 'castorena',\n",
       " 'cat',\n",
       " 'catalog',\n",
       " 'catch',\n",
       " 'catharines',\n",
       " 'cathartic',\n",
       " 'cats',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'cbd',\n",
       " 'cbt',\n",
       " 'cc',\n",
       " 'cctv',\n",
       " 'ccu',\n",
       " 'cd',\n",
       " 'cdi',\n",
       " 'ceased',\n",
       " 'celebrity',\n",
       " 'celexa',\n",
       " 'cente',\n",
       " 'center',\n",
       " 'central',\n",
       " 'centric',\n",
       " 'ceo',\n",
       " 'cephadol',\n",
       " 'cerebellum',\n",
       " 'ces',\n",
       " 'cf',\n",
       " 'cgm',\n",
       " 'ch',\n",
       " 'chad',\n",
       " 'chair',\n",
       " 'challenge',\n",
       " 'champion',\n",
       " 'chance',\n",
       " 'chanel',\n",
       " 'change',\n",
       " 'changer',\n",
       " 'changes',\n",
       " 'channel',\n",
       " 'chapter',\n",
       " 'charlotte',\n",
       " 'chart',\n",
       " 'chase',\n",
       " 'check',\n",
       " 'checkbox',\n",
       " 'checkmate',\n",
       " 'cheerful',\n",
       " 'cheers',\n",
       " 'cheese',\n",
       " 'chemo',\n",
       " 'cherprangbnk',\n",
       " 'chest',\n",
       " 'chester',\n",
       " 'chewed',\n",
       " 'chiari',\n",
       " 'chicago',\n",
       " 'chicken',\n",
       " 'chiiwii',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'children',\n",
       " 'chili',\n",
       " 'chill',\n",
       " 'chinatown',\n",
       " 'chip',\n",
       " 'chlorpheniramine',\n",
       " 'chlorpromacine',\n",
       " 'choice',\n",
       " 'choruses',\n",
       " 'chris',\n",
       " 'chrome',\n",
       " 'chronic',\n",
       " 'chronicle',\n",
       " 'chrono',\n",
       " 'church',\n",
       " 'churchill',\n",
       " 'cinematic',\n",
       " 'ciprofloxacin',\n",
       " 'circadin',\n",
       " 'citalopram',\n",
       " 'citizen',\n",
       " 'city',\n",
       " 'ck',\n",
       " 'cl',\n",
       " 'class',\n",
       " 'classroom',\n",
       " 'cleanclear',\n",
       " 'cleaning',\n",
       " 'clear',\n",
       " 'cleared',\n",
       " 'click',\n",
       " 'clickkey',\n",
       " 'clickmod',\n",
       " 'clickonly',\n",
       " 'clindamycin',\n",
       " 'clinic',\n",
       " 'clinical',\n",
       " 'clinically',\n",
       " 'clinomania',\n",
       " 'clnazepam',\n",
       " 'clo',\n",
       " 'clomipramine',\n",
       " 'clonazeoam',\n",
       " 'clonazepam',\n",
       " 'clonidine',\n",
       " 'clorazepate',\n",
       " 'close',\n",
       " 'closs',\n",
       " 'cloth',\n",
       " 'clouds',\n",
       " 'clozapine',\n",
       " 'club',\n",
       " 'cm',\n",
       " 'co',\n",
       " 'coach',\n",
       " 'cobain',\n",
       " 'cocktail',\n",
       " 'code',\n",
       " 'coding',\n",
       " 'cognitive',\n",
       " 'coined',\n",
       " 'cold',\n",
       " 'colder',\n",
       " 'coldplay',\n",
       " 'collectives',\n",
       " 'college',\n",
       " 'color',\n",
       " 'com',\n",
       " 'combination',\n",
       " 'combo',\n",
       " 'combobox',\n",
       " 'come',\n",
       " 'comedy',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'comic',\n",
       " 'comment',\n",
       " 'commit',\n",
       " 'common',\n",
       " 'commonsense',\n",
       " 'communication',\n",
       " 'community',\n",
       " 'como',\n",
       " 'company',\n",
       " 'compare',\n",
       " 'comparedocumentposition',\n",
       " 'comparedocumentpositionin',\n",
       " 'compass',\n",
       " 'competitive',\n",
       " 'complain',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'complicated',\n",
       " 'compulsion',\n",
       " 'compulsive',\n",
       " 'compulsivedisorder',\n",
       " 'conazepam',\n",
       " 'concat',\n",
       " 'concentrate',\n",
       " 'concern',\n",
       " 'concerta',\n",
       " 'concluding',\n",
       " 'concor',\n",
       " 'concussion',\n",
       " 'condition',\n",
       " 'confession',\n",
       " 'confide',\n",
       " 'confidence',\n",
       " 'confirm',\n",
       " 'conflicts',\n",
       " 'confort',\n",
       " 'congestive',\n",
       " 'connect',\n",
       " 'connection',\n",
       " 'connective',\n",
       " 'consciousness',\n",
       " 'consent',\n",
       " 'conservative',\n",
       " 'conspiracy',\n",
       " 'constantly',\n",
       " 'consultant',\n",
       " 'consumed',\n",
       " 'consumer',\n",
       " 'contact',\n",
       " 'contagion',\n",
       " 'contagious',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'containsin',\n",
       " 'content',\n",
       " 'continue',\n",
       " 'continued',\n",
       " 'control',\n",
       " 'controller',\n",
       " 'conversation',\n",
       " 'cook',\n",
       " 'cool',\n",
       " 'copy',\n",
       " 'core',\n",
       " 'cornell',\n",
       " 'cortisol',\n",
       " 'cost',\n",
       " 'cosult',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'counselling',\n",
       " 'counselor',\n",
       " 'count',\n",
       " 'countdown',\n",
       " 'counter',\n",
       " 'courage',\n",
       " 'cover',\n",
       " 'coverdanc',\n",
       " 'covering',\n",
       " 'covert',\n",
       " 'covid',\n",
       " 'cp',\n",
       " 'cpr',\n",
       " 'cpun',\n",
       " 'cpz',\n",
       " 'cr',\n",
       " 'cranberries',\n",
       " 'cranial',\n",
       " 'crazy',\n",
       " 'createevent',\n",
       " 'createeventobject',\n",
       " 'creative',\n",
       " 'creator',\n",
       " 'credit',\n",
       " 'crews',\n",
       " 'cri',\n",
       " 'crimsonher',\n",
       " 'crisis',\n",
       " 'critical',\n",
       " 'criticism',\n",
       " 'crono',\n",
       " 'crossfit',\n",
       " 'crowd',\n",
       " 'crush',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'crypto',\n",
       " 'cs',\n",
       " 'cshid',\n",
       " 'csr',\n",
       " 'ct',\n",
       " 'ctrlkey',\n",
       " 'cu',\n",
       " 'culter',\n",
       " 'culture',\n",
       " 'culverhouse',\n",
       " 'cup',\n",
       " 'cups',\n",
       " 'cures',\n",
       " 'curious',\n",
       " 'current',\n",
       " 'cursor',\n",
       " 'curve',\n",
       " 'custody',\n",
       " 'custom',\n",
       " 'customer',\n",
       " 'cut',\n",
       " 'cuts',\n",
       " 'cv',\n",
       " 'cyber',\n",
       " 'cyberbully',\n",
       " 'cyberbullying',\n",
       " 'cybrrbullying',\n",
       " 'cycle',\n",
       " 'cymbalta',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'dafalgan',\n",
       " 'dale',\n",
       " 'damage',\n",
       " 'dance',\n",
       " 'dandism',\n",
       " 'daniel',\n",
       " 'danielle',\n",
       " 'dark',\n",
       " 'dass',\n",
       " 'data',\n",
       " 'database',\n",
       " 'date',\n",
       " 'datetime',\n",
       " 'david',\n",
       " 'dawn',\n",
       " 'day',\n",
       " 'daydreaming',\n",
       " 'days',\n",
       " 'dazai',\n",
       " 'daze',\n",
       " 'dc',\n",
       " 'dead',\n",
       " 'deadline',\n",
       " 'deal',\n",
       " 'dean',\n",
       " 'deanxit',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'dec',\n",
       " 'december',\n",
       " 'declarative',\n",
       " 'declare',\n",
       " 'decorative',\n",
       " 'decreased',\n",
       " 'deduct',\n",
       " 'deep',\n",
       " 'deeply',\n",
       " 'default',\n",
       " 'defence',\n",
       " 'defense',\n",
       " 'defenseless',\n",
       " 'deficit',\n",
       " 'degree',\n",
       " 'dek',\n",
       " 'delay',\n",
       " 'delete',\n",
       " 'deleted',\n",
       " 'delta',\n",
       " 'delusion',\n",
       " 'dementia',\n",
       " 'denial',\n",
       " 'dental',\n",
       " 'denuvo',\n",
       " 'depakin',\n",
       " 'depakine',\n",
       " 'depakins',\n",
       " 'depeche',\n",
       " 'depend',\n",
       " 'depersonalisation',\n",
       " 'depersonalization',\n",
       " 'depress',\n",
       " 'depressant',\n",
       " 'depressed',\n",
       " 'depressing',\n",
       " 'depression',\n",
       " 'depressive',\n",
       " 'depresssed',\n",
       " 'deproxin',\n",
       " 'derealization',\n",
       " 'dermatitis',\n",
       " 'describing',\n",
       " 'desensitization',\n",
       " 'desert',\n",
       " 'deserve',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'designer',\n",
       " 'desire',\n",
       " 'desired',\n",
       " 'despression',\n",
       " 'destination',\n",
       " 'destroy',\n",
       " 'destroyed',\n",
       " 'detail',\n",
       " 'details',\n",
       " 'detective',\n",
       " 'deth',\n",
       " 'detox',\n",
       " 'dev',\n",
       " 'devalue',\n",
       " 'develop',\n",
       " 'development',\n",
       " 'deviation',\n",
       " 'devill',\n",
       " 'devoid',\n",
       " 'dhc',\n",
       " 'dheas',\n",
       " 'di',\n",
       " 'diagnose',\n",
       " 'diagnosed',\n",
       " 'diagnostic',\n",
       " 'dialog',\n",
       " 'diamond',\n",
       " 'diaper',\n",
       " 'diapotassium',\n",
       " 'diary',\n",
       " 'diazepam',\n",
       " 'dictates',\n",
       " 'dictionary',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'diet',\n",
       " 'diezepam',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'difficulty',\n",
       " 'dipotassium',\n",
       " 'direct',\n",
       " 'directed',\n",
       " 'director',\n",
       " 'dirty',\n",
       " 'disabled',\n",
       " 'disappointed',\n",
       " 'disappointment',\n",
       " 'discipline',\n",
       " 'discomfort',\n",
       " 'disconnect',\n",
       " 'disconnected',\n",
       " 'discord',\n",
       " 'discredit',\n",
       " 'discuss',\n",
       " 'disease',\n",
       " 'disgrace',\n",
       " 'disoder',\n",
       " 'disorder',\n",
       " 'disorders',\n",
       " 'displacement',\n",
       " 'dissociative',\n",
       " 'dissorder',\n",
       " 'distance',\n",
       " 'distancing',\n",
       " 'dive',\n",
       " 'diving',\n",
       " 'divorced',\n",
       " 'diy',\n",
       " 'dizepam',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 6: match the X_all columns with X_train_sm_columns, \n",
    "# so we can use X_all to predict the new data, using the trained model 'pantip_post_model_train_simplified_label_lr.pkl'\n",
    "\n",
    "# check the shape of X_sm and X_train_sm_columns\n",
    "print(X_all.shape)\n",
    "X_train_sm_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13360, 17568)\n"
     ]
    }
   ],
   "source": [
    "# match the X_all.columns with columns_names_list \n",
    "# because columns_names_list is the list of columns of X_train_sm_columns which is used to train the model 'pantip_post_model_train_simplified_label_lr.pkl'\n",
    "# column_names_list is the feature list of the trained model 'pantip_post_model_train_simplified_label_lr.pkl'\n",
    "# x_all number of columns should the same as column_names_list's length\n",
    "\n",
    "# 1. Adding missing columns from column_names_list to X_sm and imputing with 0\n",
    "for col in X_train_sm_columns:\n",
    "    if col not in X_all.columns:\n",
    "        X_all[col] = 0\n",
    "\n",
    "# 2. Dropping columns in X_sm that are not in column_names_list\n",
    "X_all = X_all[X_train_sm_columns]\n",
    "\n",
    "# This results in X_sm having the same columns as column_names_list, in the same order.\n",
    "# Missing columns are added and imputed with 0, and extra columns are dropped.\n",
    "\n",
    "# check the shape of X_all\n",
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check whether all column names in X_sm are strings. If all are strings, it will return true\n",
    "print(X_all.columns.dtype == 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_len</th>\n",
       "      <th>__</th>\n",
       "      <th>___</th>\n",
       "      <th>____</th>\n",
       "      <th>_____</th>\n",
       "      <th>________</th>\n",
       "      <th>_________</th>\n",
       "      <th>__________</th>\n",
       "      <th>____________</th>\n",
       "      <th>__________________</th>\n",
       "      <th>...</th>\n",
       "      <th>year_2022</th>\n",
       "      <th>year_2023</th>\n",
       "      <th>time_day_12-15</th>\n",
       "      <th>time_day_15-18</th>\n",
       "      <th>time_day_18-21</th>\n",
       "      <th>time_day_21-24</th>\n",
       "      <th>time_day_3-6</th>\n",
       "      <th>time_day_6-9</th>\n",
       "      <th>time_day_9-12</th>\n",
       "      <th>text_emoji_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.768740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.672877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.765545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.747171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.766343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13355</th>\n",
       "      <td>-0.679268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13356</th>\n",
       "      <td>-0.517898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13357</th>\n",
       "      <td>-0.541864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13358</th>\n",
       "      <td>-0.211935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13359</th>\n",
       "      <td>-0.696843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13360 rows × 17568 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_len   __  ___  ____  _____  ________  _________  __________  \\\n",
       "0     -0.768740  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "1     -0.672877  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "2     -0.765545  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "3     -0.747171  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "4     -0.766343  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "...         ...  ...  ...   ...    ...       ...        ...         ...   \n",
       "13355 -0.679268  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "13356 -0.517898  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "13357 -0.541864  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "13358 -0.211935  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "13359 -0.696843  0.0  0.0   0.0    0.0       0.0        0.0         0.0   \n",
       "\n",
       "       ____________  __________________  ...  year_2022  year_2023  \\\n",
       "0               0.0                 0.0  ...          0          1   \n",
       "1               0.0                 0.0  ...          0          1   \n",
       "2               0.0                 0.0  ...          0          1   \n",
       "3               0.0                 0.0  ...          0          1   \n",
       "4               0.0                 0.0  ...          0          1   \n",
       "...             ...                 ...  ...        ...        ...   \n",
       "13355           0.0                 0.0  ...          1          0   \n",
       "13356           0.0                 0.0  ...          1          0   \n",
       "13357           0.0                 0.0  ...          0          0   \n",
       "13358           0.0                 0.0  ...          0          1   \n",
       "13359           0.0                 0.0  ...          0          1   \n",
       "\n",
       "       time_day_12-15  time_day_15-18  time_day_18-21  time_day_21-24  \\\n",
       "0                   0               0               0               0   \n",
       "1                   0               0               0               1   \n",
       "2                   0               0               0               0   \n",
       "3                   0               0               0               0   \n",
       "4                   0               0               0               0   \n",
       "...               ...             ...             ...             ...   \n",
       "13355               0               0               0               1   \n",
       "13356               0               0               0               1   \n",
       "13357               1               0               0               0   \n",
       "13358               0               0               0               1   \n",
       "13359               0               0               1               0   \n",
       "\n",
       "       time_day_3-6  time_day_6-9  time_day_9-12  text_emoji_1  \n",
       "0                 0             0              1             0  \n",
       "1                 0             0              0             0  \n",
       "2                 0             0              1             1  \n",
       "3                 1             0              0             0  \n",
       "4                 1             0              0             0  \n",
       "...             ...           ...            ...           ...  \n",
       "13355             0             0              0             0  \n",
       "13356             0             0              0             0  \n",
       "13357             0             0              0             0  \n",
       "13358             0             0              0             0  \n",
       "13359             0             0              0             0  \n",
       "\n",
       "[13360 rows x 17568 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **----- Use the trained model to test the data -----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the trained model to test the processed data X_sm and y_sm\n",
    "y_pred = pantip_post_model_train_simplified_label_lr.predict(X_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9449850299401198\n",
      "precision score:  0.3379396984924623\n",
      "recall score:  0.5639412997903563\n",
      "f1 score:  0.42262372348782407\n",
      "roc auc score:  0.7615173393308686\n",
      "confusion matrix:  [[12356   527]\n",
      " [  208   269]]\n",
      "classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     12883\n",
      "           1       0.34      0.56      0.42       477\n",
      "\n",
      "    accuracy                           0.94     13360\n",
      "   macro avg       0.66      0.76      0.70     13360\n",
      "weighted avg       0.96      0.94      0.95     13360\n",
      "\n",
      "roc curve:  (array([0.        , 0.04090662, 1.        ]), array([0.       , 0.5639413, 1.       ]), array([inf,  1.,  0.]))\n"
     ]
    }
   ],
   "source": [
    "#evaluate the test model using the metrics including accuracy score, precision score, recall score, f1 score, roc auc score, confusion matrix as well as classification report, roc curve\n",
    "print('accuracy score: ', accuracy_score(y, y_pred))\n",
    "print('precision score: ', precision_score(y, y_pred))\n",
    "print('recall score: ', recall_score(y, y_pred))\n",
    "print('f1 score: ', f1_score(y, y_pred))\n",
    "print('roc auc score: ', roc_auc_score(y, y_pred))\n",
    "print('confusion matrix: ', confusion_matrix(y, y_pred))\n",
    "print('classification report: ', classification_report(y, y_pred))\n",
    "print('roc curve: ', roc_curve(y, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
